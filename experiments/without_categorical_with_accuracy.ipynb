{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2022Group3/projectNotebooks/blob/main/without_categorical_with_accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glekdw9QYM7R"
      },
      "outputs": [],
      "source": [
        "# baseline model with dropout and data augmentation on the cifar10 dataset\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_WXoAU7YYi-",
        "outputId": "72fb511a-e415-488c-bb68-8c67012d360c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "  data=np.load(r'drive/MyDrive/data_modified.npz')\n",
        "  data=dict(zip((\"{}\".format(k) for k in data),(data[k] for k in data)))\n",
        "  trainX=data['train']\n",
        "  trainy=data['ytrain']\n",
        "  validationX=data['validation']\n",
        "  validationy=data['yvalidation']\n",
        "  testX=data['test']\n",
        "  testy=data['ytest']\n",
        "  return trainX,trainy,validationX,validationy,testX,testy"
      ],
      "metadata": {
        "id": "VX6_KJutYj_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d,e,f=load_dataset()"
      ],
      "metadata": {
        "id": "p3JQ7hjLfE8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.shape\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXwFlEPMgz5n",
        "outputId": "78d618c6-bfaa-440c-c826-87c2c8906696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 0, 9, ..., 7, 8, 4], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxhQd_sUfMsF",
        "outputId": "a60556ea-5c18-495a-f18b-b75850b3d9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45000"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hZo69ukfQjH",
        "outputId": "18df80e1-2ddb-45a9-8804-5018e3df571b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scale pixels\n",
        "def prep_pixels(train,validation):\n",
        "  # convert from integers to floats\n",
        "  train_norm = train.astype('float32')\n",
        "  validation_norm = validation.astype('float32')\n",
        "  # normalize to range 0-1\n",
        "  train_norm = train_norm / 255.0\n",
        "  validation_norm = validation / 255.0\n",
        "  # return normalized images\n",
        "  return train_norm, validation_norm"
      ],
      "metadata": {
        "id": "hotdCojsYpI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # list all data in history\n",
        "    print(history.history.keys())"
      ],
      "metadata": {
        "id": "XJGrRoVsYqyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cnn model\n",
        "def define_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(28, activation='softmax'))\n",
        "\t# compile model\n",
        "  opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "  # model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  # model.compile(optimizer=opt,loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "PbaYP_yKYvZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='validation')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='validation')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\n"
      ],
      "metadata": {
        "id": "h5sKaXpiY07G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "  trainX, trainy,validationX,validationy, testX, testy = load_dataset()\n",
        "\t# prepare pixel data\n",
        "  trainX, validationX = prep_pixels(trainX, validationX)\n",
        "\t# define model\n",
        "  model = define_model()\n",
        "\t# create data generator\n",
        "  datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "\t# prepare iterator\n",
        "  it_train = datagen.flow(trainX, trainy, batch_size=64)\n",
        "\t# fit model\n",
        "  steps = int(trainX.shape[0] / 64)\n",
        "  history = model.fit(it_train, steps_per_epoch=steps, epochs=250, validation_data=(validationX, validationy), verbose=1)\n",
        "\t# evaluate model\n",
        "  _, acc = model.evaluate(testX, testy, verbose=1)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "  summarize_diagnostics(history)\n",
        "\t#save model\n",
        "  model.save('/content/drive/MyDrive/model2.h5')\n",
        "  return model"
      ],
      "metadata": {
        "id": "hjm8k2rwY6rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entry point, run the test harness\n",
        "model=run_test_harness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dSBBUTivY8kH",
        "outputId": "97c7e588-3b7a-4a8f-c962-80c351ff4e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "703/703 [==============================] - 36s 34ms/step - loss: 2.8622 - accuracy: 0.2416 - val_loss: 1.9587 - val_accuracy: 0.3825\n",
            "Epoch 2/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 2.1599 - accuracy: 0.3445 - val_loss: 1.8186 - val_accuracy: 0.4208\n",
            "Epoch 3/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 1.9485 - accuracy: 0.3873 - val_loss: 1.6977 - val_accuracy: 0.4405\n",
            "Epoch 4/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.8107 - accuracy: 0.4155 - val_loss: 1.7268 - val_accuracy: 0.4511\n",
            "Epoch 5/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.7314 - accuracy: 0.4392 - val_loss: 1.6069 - val_accuracy: 0.4760\n",
            "Epoch 6/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6588 - accuracy: 0.4560 - val_loss: 1.5534 - val_accuracy: 0.5013\n",
            "Epoch 7/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 1.5940 - accuracy: 0.4735 - val_loss: 1.5538 - val_accuracy: 0.4944\n",
            "Epoch 8/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.5448 - accuracy: 0.4906 - val_loss: 1.4619 - val_accuracy: 0.5279\n",
            "Epoch 9/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.4980 - accuracy: 0.5066 - val_loss: 1.6223 - val_accuracy: 0.4785\n",
            "Epoch 10/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.4587 - accuracy: 0.5204 - val_loss: 1.5076 - val_accuracy: 0.5120\n",
            "Epoch 11/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.4293 - accuracy: 0.5274 - val_loss: 1.4832 - val_accuracy: 0.5231\n",
            "Epoch 12/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.3968 - accuracy: 0.5394 - val_loss: 1.3922 - val_accuracy: 0.5453\n",
            "Epoch 13/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 1.3693 - accuracy: 0.5496 - val_loss: 1.5129 - val_accuracy: 0.5079\n",
            "Epoch 14/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.3487 - accuracy: 0.5560 - val_loss: 1.3855 - val_accuracy: 0.5507\n",
            "Epoch 15/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.3204 - accuracy: 0.5664 - val_loss: 1.2767 - val_accuracy: 0.5840\n",
            "Epoch 16/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.2936 - accuracy: 0.5715 - val_loss: 1.2476 - val_accuracy: 0.5911\n",
            "Epoch 17/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 1.2735 - accuracy: 0.5804 - val_loss: 1.2252 - val_accuracy: 0.5937\n",
            "Epoch 18/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2570 - accuracy: 0.5866 - val_loss: 1.2909 - val_accuracy: 0.5737\n",
            "Epoch 19/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2385 - accuracy: 0.5923 - val_loss: 1.3503 - val_accuracy: 0.5723\n",
            "Epoch 20/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2224 - accuracy: 0.6000 - val_loss: 1.1300 - val_accuracy: 0.6267\n",
            "Epoch 21/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2062 - accuracy: 0.6035 - val_loss: 1.1055 - val_accuracy: 0.6359\n",
            "Epoch 22/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.1853 - accuracy: 0.6092 - val_loss: 1.3103 - val_accuracy: 0.5779\n",
            "Epoch 23/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.1709 - accuracy: 0.6154 - val_loss: 1.1736 - val_accuracy: 0.6220\n",
            "Epoch 24/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.1528 - accuracy: 0.6234 - val_loss: 1.2342 - val_accuracy: 0.5960\n",
            "Epoch 25/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.1433 - accuracy: 0.6226 - val_loss: 1.1115 - val_accuracy: 0.6396\n",
            "Epoch 26/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.1325 - accuracy: 0.6288 - val_loss: 1.1501 - val_accuracy: 0.6212\n",
            "Epoch 27/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.1160 - accuracy: 0.6313 - val_loss: 1.1295 - val_accuracy: 0.6357\n",
            "Epoch 28/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.1062 - accuracy: 0.6352 - val_loss: 1.0206 - val_accuracy: 0.6653\n",
            "Epoch 29/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0932 - accuracy: 0.6422 - val_loss: 1.0279 - val_accuracy: 0.6609\n",
            "Epoch 30/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 1.0827 - accuracy: 0.6464 - val_loss: 0.9928 - val_accuracy: 0.6741\n",
            "Epoch 31/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.0712 - accuracy: 0.6504 - val_loss: 1.0522 - val_accuracy: 0.6620\n",
            "Epoch 32/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.0587 - accuracy: 0.6533 - val_loss: 0.9975 - val_accuracy: 0.6733\n",
            "Epoch 33/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0506 - accuracy: 0.6540 - val_loss: 0.9638 - val_accuracy: 0.6825\n",
            "Epoch 34/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 1.0354 - accuracy: 0.6586 - val_loss: 1.0005 - val_accuracy: 0.6731\n",
            "Epoch 35/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0297 - accuracy: 0.6621 - val_loss: 0.9554 - val_accuracy: 0.6825\n",
            "Epoch 36/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0256 - accuracy: 0.6658 - val_loss: 0.8949 - val_accuracy: 0.7043\n",
            "Epoch 37/250\n",
            "703/703 [==============================] - 23s 32ms/step - loss: 1.0171 - accuracy: 0.6663 - val_loss: 0.9488 - val_accuracy: 0.6895\n",
            "Epoch 38/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 1.0086 - accuracy: 0.6691 - val_loss: 1.0400 - val_accuracy: 0.6701\n",
            "Epoch 39/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9883 - accuracy: 0.6742 - val_loss: 0.9031 - val_accuracy: 0.7035\n",
            "Epoch 40/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.9861 - accuracy: 0.6771 - val_loss: 0.9135 - val_accuracy: 0.6944\n",
            "Epoch 41/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.9814 - accuracy: 0.6791 - val_loss: 0.8709 - val_accuracy: 0.7144\n",
            "Epoch 42/250\n",
            "703/703 [==============================] - 23s 32ms/step - loss: 0.9696 - accuracy: 0.6811 - val_loss: 0.9410 - val_accuracy: 0.6905\n",
            "Epoch 43/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9642 - accuracy: 0.6835 - val_loss: 0.8891 - val_accuracy: 0.7055\n",
            "Epoch 44/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.9537 - accuracy: 0.6900 - val_loss: 0.8690 - val_accuracy: 0.7101\n",
            "Epoch 45/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9480 - accuracy: 0.6887 - val_loss: 0.8780 - val_accuracy: 0.7077\n",
            "Epoch 46/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9376 - accuracy: 0.6935 - val_loss: 0.8488 - val_accuracy: 0.7176\n",
            "Epoch 47/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9345 - accuracy: 0.6934 - val_loss: 0.9038 - val_accuracy: 0.7027\n",
            "Epoch 48/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.9219 - accuracy: 0.6971 - val_loss: 0.8398 - val_accuracy: 0.7240\n",
            "Epoch 49/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9153 - accuracy: 0.7015 - val_loss: 0.8583 - val_accuracy: 0.7201\n",
            "Epoch 50/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9135 - accuracy: 0.7027 - val_loss: 0.8176 - val_accuracy: 0.7309\n",
            "Epoch 51/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.9068 - accuracy: 0.7037 - val_loss: 0.8632 - val_accuracy: 0.7164\n",
            "Epoch 52/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.9044 - accuracy: 0.7062 - val_loss: 0.8801 - val_accuracy: 0.7136\n",
            "Epoch 53/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8905 - accuracy: 0.7099 - val_loss: 0.8112 - val_accuracy: 0.7315\n",
            "Epoch 54/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8941 - accuracy: 0.7102 - val_loss: 0.8133 - val_accuracy: 0.7324\n",
            "Epoch 55/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8796 - accuracy: 0.7135 - val_loss: 0.8496 - val_accuracy: 0.7245\n",
            "Epoch 56/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8710 - accuracy: 0.7153 - val_loss: 0.8483 - val_accuracy: 0.7213\n",
            "Epoch 57/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8745 - accuracy: 0.7127 - val_loss: 0.8090 - val_accuracy: 0.7345\n",
            "Epoch 58/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8663 - accuracy: 0.7162 - val_loss: 0.8627 - val_accuracy: 0.7187\n",
            "Epoch 59/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8567 - accuracy: 0.7207 - val_loss: 0.8076 - val_accuracy: 0.7343\n",
            "Epoch 60/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8576 - accuracy: 0.7219 - val_loss: 0.8332 - val_accuracy: 0.7260\n",
            "Epoch 61/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8455 - accuracy: 0.7239 - val_loss: 0.7594 - val_accuracy: 0.7481\n",
            "Epoch 62/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8405 - accuracy: 0.7271 - val_loss: 0.7509 - val_accuracy: 0.7544\n",
            "Epoch 63/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.8386 - accuracy: 0.7266 - val_loss: 0.7473 - val_accuracy: 0.7553\n",
            "Epoch 64/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8323 - accuracy: 0.7282 - val_loss: 0.7837 - val_accuracy: 0.7437\n",
            "Epoch 65/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8327 - accuracy: 0.7279 - val_loss: 0.7763 - val_accuracy: 0.7428\n",
            "Epoch 66/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.8221 - accuracy: 0.7309 - val_loss: 0.7073 - val_accuracy: 0.7664\n",
            "Epoch 67/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8178 - accuracy: 0.7338 - val_loss: 0.7831 - val_accuracy: 0.7476\n",
            "Epoch 68/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8119 - accuracy: 0.7353 - val_loss: 0.7869 - val_accuracy: 0.7463\n",
            "Epoch 69/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.8138 - accuracy: 0.7364 - val_loss: 0.7725 - val_accuracy: 0.7483\n",
            "Epoch 70/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8076 - accuracy: 0.7360 - val_loss: 0.7423 - val_accuracy: 0.7565\n",
            "Epoch 71/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7981 - accuracy: 0.7409 - val_loss: 0.7180 - val_accuracy: 0.7644\n",
            "Epoch 72/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.7946 - accuracy: 0.7411 - val_loss: 0.7445 - val_accuracy: 0.7559\n",
            "Epoch 73/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7900 - accuracy: 0.7432 - val_loss: 0.7001 - val_accuracy: 0.7696\n",
            "Epoch 74/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7883 - accuracy: 0.7417 - val_loss: 0.6770 - val_accuracy: 0.7797\n",
            "Epoch 75/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7830 - accuracy: 0.7438 - val_loss: 0.7193 - val_accuracy: 0.7668\n",
            "Epoch 76/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7766 - accuracy: 0.7462 - val_loss: 0.7332 - val_accuracy: 0.7635\n",
            "Epoch 77/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7753 - accuracy: 0.7480 - val_loss: 0.7709 - val_accuracy: 0.7523\n",
            "Epoch 78/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7705 - accuracy: 0.7516 - val_loss: 0.7521 - val_accuracy: 0.7575\n",
            "Epoch 79/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.7718 - accuracy: 0.7487 - val_loss: 0.6751 - val_accuracy: 0.7751\n",
            "Epoch 80/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7657 - accuracy: 0.7529 - val_loss: 0.7356 - val_accuracy: 0.7613\n",
            "Epoch 81/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7614 - accuracy: 0.7522 - val_loss: 0.6768 - val_accuracy: 0.7767\n",
            "Epoch 82/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7591 - accuracy: 0.7514 - val_loss: 0.6843 - val_accuracy: 0.7789\n",
            "Epoch 83/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.7564 - accuracy: 0.7557 - val_loss: 0.6892 - val_accuracy: 0.7805\n",
            "Epoch 84/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7546 - accuracy: 0.7543 - val_loss: 0.7213 - val_accuracy: 0.7644\n",
            "Epoch 85/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7447 - accuracy: 0.7557 - val_loss: 0.6436 - val_accuracy: 0.7915\n",
            "Epoch 86/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7423 - accuracy: 0.7585 - val_loss: 0.6560 - val_accuracy: 0.7856\n",
            "Epoch 87/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.7367 - accuracy: 0.7615 - val_loss: 0.6696 - val_accuracy: 0.7816\n",
            "Epoch 88/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7408 - accuracy: 0.7601 - val_loss: 0.7020 - val_accuracy: 0.7699\n",
            "Epoch 89/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7356 - accuracy: 0.7602 - val_loss: 0.6510 - val_accuracy: 0.7903\n",
            "Epoch 90/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7315 - accuracy: 0.7609 - val_loss: 0.6547 - val_accuracy: 0.7897\n",
            "Epoch 91/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.7306 - accuracy: 0.7613 - val_loss: 0.6281 - val_accuracy: 0.7939\n",
            "Epoch 92/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7208 - accuracy: 0.7656 - val_loss: 0.6224 - val_accuracy: 0.7987\n",
            "Epoch 93/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7273 - accuracy: 0.7652 - val_loss: 0.6522 - val_accuracy: 0.7899\n",
            "Epoch 94/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7177 - accuracy: 0.7649 - val_loss: 0.6828 - val_accuracy: 0.7792\n",
            "Epoch 95/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7190 - accuracy: 0.7686 - val_loss: 0.6493 - val_accuracy: 0.7877\n",
            "Epoch 96/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.7115 - accuracy: 0.7685 - val_loss: 0.6325 - val_accuracy: 0.7947\n",
            "Epoch 97/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7104 - accuracy: 0.7690 - val_loss: 0.6280 - val_accuracy: 0.7967\n",
            "Epoch 98/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7023 - accuracy: 0.7703 - val_loss: 0.6460 - val_accuracy: 0.7905\n",
            "Epoch 99/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7087 - accuracy: 0.7700 - val_loss: 0.6347 - val_accuracy: 0.7980\n",
            "Epoch 100/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.7015 - accuracy: 0.7715 - val_loss: 0.6737 - val_accuracy: 0.7840\n",
            "Epoch 101/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6984 - accuracy: 0.7748 - val_loss: 0.6664 - val_accuracy: 0.7893\n",
            "Epoch 102/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6910 - accuracy: 0.7756 - val_loss: 0.6661 - val_accuracy: 0.7868\n",
            "Epoch 103/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6935 - accuracy: 0.7748 - val_loss: 0.6283 - val_accuracy: 0.7983\n",
            "Epoch 104/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6864 - accuracy: 0.7780 - val_loss: 0.6498 - val_accuracy: 0.7917\n",
            "Epoch 105/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6869 - accuracy: 0.7742 - val_loss: 0.6139 - val_accuracy: 0.8007\n",
            "Epoch 106/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6845 - accuracy: 0.7784 - val_loss: 0.6894 - val_accuracy: 0.7792\n",
            "Epoch 107/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6803 - accuracy: 0.7782 - val_loss: 0.6036 - val_accuracy: 0.8056\n",
            "Epoch 108/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6732 - accuracy: 0.7814 - val_loss: 0.5932 - val_accuracy: 0.8092\n",
            "Epoch 109/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6770 - accuracy: 0.7794 - val_loss: 0.5872 - val_accuracy: 0.8112\n",
            "Epoch 110/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6800 - accuracy: 0.7807 - val_loss: 0.6449 - val_accuracy: 0.7947\n",
            "Epoch 111/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.6709 - accuracy: 0.7827 - val_loss: 0.6280 - val_accuracy: 0.8009\n",
            "Epoch 112/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.6730 - accuracy: 0.7812 - val_loss: 0.6281 - val_accuracy: 0.7981\n",
            "Epoch 113/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.6690 - accuracy: 0.7822 - val_loss: 0.6171 - val_accuracy: 0.8036\n",
            "Epoch 114/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6638 - accuracy: 0.7856 - val_loss: 0.5621 - val_accuracy: 0.8195\n",
            "Epoch 115/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6588 - accuracy: 0.7844 - val_loss: 0.5895 - val_accuracy: 0.8084\n",
            "Epoch 116/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6522 - accuracy: 0.7877 - val_loss: 0.6372 - val_accuracy: 0.7957\n",
            "Epoch 117/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6647 - accuracy: 0.7844 - val_loss: 0.5781 - val_accuracy: 0.8165\n",
            "Epoch 118/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6553 - accuracy: 0.7862 - val_loss: 0.5992 - val_accuracy: 0.8067\n",
            "Epoch 119/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6553 - accuracy: 0.7885 - val_loss: 0.6047 - val_accuracy: 0.8048\n",
            "Epoch 120/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6457 - accuracy: 0.7905 - val_loss: 0.6409 - val_accuracy: 0.7948\n",
            "Epoch 121/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6476 - accuracy: 0.7882 - val_loss: 0.6250 - val_accuracy: 0.7996\n",
            "Epoch 122/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6540 - accuracy: 0.7870 - val_loss: 0.6138 - val_accuracy: 0.8000\n",
            "Epoch 123/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6482 - accuracy: 0.7896 - val_loss: 0.5713 - val_accuracy: 0.8165\n",
            "Epoch 124/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6427 - accuracy: 0.7911 - val_loss: 0.5826 - val_accuracy: 0.8129\n",
            "Epoch 125/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6398 - accuracy: 0.7909 - val_loss: 0.5735 - val_accuracy: 0.8168\n",
            "Epoch 126/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6388 - accuracy: 0.7930 - val_loss: 0.6208 - val_accuracy: 0.8023\n",
            "Epoch 127/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6416 - accuracy: 0.7909 - val_loss: 0.5779 - val_accuracy: 0.8141\n",
            "Epoch 128/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6414 - accuracy: 0.7923 - val_loss: 0.5640 - val_accuracy: 0.8175\n",
            "Epoch 129/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6339 - accuracy: 0.7936 - val_loss: 0.6022 - val_accuracy: 0.8077\n",
            "Epoch 130/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6326 - accuracy: 0.7944 - val_loss: 0.6040 - val_accuracy: 0.8065\n",
            "Epoch 131/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.6265 - accuracy: 0.7963 - val_loss: 0.5596 - val_accuracy: 0.8228\n",
            "Epoch 132/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6250 - accuracy: 0.7983 - val_loss: 0.5858 - val_accuracy: 0.8116\n",
            "Epoch 133/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6309 - accuracy: 0.7950 - val_loss: 0.5786 - val_accuracy: 0.8168\n",
            "Epoch 134/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6263 - accuracy: 0.7960 - val_loss: 0.5487 - val_accuracy: 0.8233\n",
            "Epoch 135/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6180 - accuracy: 0.7999 - val_loss: 0.5675 - val_accuracy: 0.8196\n",
            "Epoch 136/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.6171 - accuracy: 0.7979 - val_loss: 0.6051 - val_accuracy: 0.8061\n",
            "Epoch 137/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.6125 - accuracy: 0.8023 - val_loss: 0.6065 - val_accuracy: 0.8083\n",
            "Epoch 138/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.6123 - accuracy: 0.8028 - val_loss: 0.6218 - val_accuracy: 0.7999\n",
            "Epoch 139/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6171 - accuracy: 0.8002 - val_loss: 0.5424 - val_accuracy: 0.8284\n",
            "Epoch 140/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.6075 - accuracy: 0.8033 - val_loss: 0.5607 - val_accuracy: 0.8239\n",
            "Epoch 141/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6085 - accuracy: 0.8005 - val_loss: 0.5599 - val_accuracy: 0.8195\n",
            "Epoch 142/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6042 - accuracy: 0.8042 - val_loss: 0.5925 - val_accuracy: 0.8123\n",
            "Epoch 143/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.6069 - accuracy: 0.8033 - val_loss: 0.5567 - val_accuracy: 0.8215\n",
            "Epoch 144/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6064 - accuracy: 0.8033 - val_loss: 0.5814 - val_accuracy: 0.8181\n",
            "Epoch 145/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6121 - accuracy: 0.8010 - val_loss: 0.5442 - val_accuracy: 0.8217\n",
            "Epoch 146/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5956 - accuracy: 0.8075 - val_loss: 0.5872 - val_accuracy: 0.8148\n",
            "Epoch 147/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5963 - accuracy: 0.8060 - val_loss: 0.5849 - val_accuracy: 0.8125\n",
            "Epoch 148/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5960 - accuracy: 0.8066 - val_loss: 0.5894 - val_accuracy: 0.8129\n",
            "Epoch 149/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5915 - accuracy: 0.8069 - val_loss: 0.5664 - val_accuracy: 0.8185\n",
            "Epoch 150/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5905 - accuracy: 0.8089 - val_loss: 0.5520 - val_accuracy: 0.8212\n",
            "Epoch 151/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5916 - accuracy: 0.8060 - val_loss: 0.5756 - val_accuracy: 0.8188\n",
            "Epoch 152/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5926 - accuracy: 0.8064 - val_loss: 0.5327 - val_accuracy: 0.8275\n",
            "Epoch 153/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5875 - accuracy: 0.8091 - val_loss: 0.5662 - val_accuracy: 0.8196\n",
            "Epoch 154/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5885 - accuracy: 0.8096 - val_loss: 0.5437 - val_accuracy: 0.8261\n",
            "Epoch 155/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5845 - accuracy: 0.8100 - val_loss: 0.5190 - val_accuracy: 0.8320\n",
            "Epoch 156/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5858 - accuracy: 0.8092 - val_loss: 0.5682 - val_accuracy: 0.8204\n",
            "Epoch 157/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5858 - accuracy: 0.8089 - val_loss: 0.5218 - val_accuracy: 0.8328\n",
            "Epoch 158/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5767 - accuracy: 0.8143 - val_loss: 0.5597 - val_accuracy: 0.8212\n",
            "Epoch 159/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5835 - accuracy: 0.8104 - val_loss: 0.5484 - val_accuracy: 0.8211\n",
            "Epoch 160/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5764 - accuracy: 0.8115 - val_loss: 0.5571 - val_accuracy: 0.8227\n",
            "Epoch 161/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5819 - accuracy: 0.8111 - val_loss: 0.5493 - val_accuracy: 0.8223\n",
            "Epoch 162/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5753 - accuracy: 0.8120 - val_loss: 0.5453 - val_accuracy: 0.8251\n",
            "Epoch 163/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.5741 - accuracy: 0.8119 - val_loss: 0.5348 - val_accuracy: 0.8271\n",
            "Epoch 164/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5722 - accuracy: 0.8129 - val_loss: 0.5192 - val_accuracy: 0.8323\n",
            "Epoch 165/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5719 - accuracy: 0.8154 - val_loss: 0.5329 - val_accuracy: 0.8284\n",
            "Epoch 166/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5719 - accuracy: 0.8125 - val_loss: 0.5524 - val_accuracy: 0.8236\n",
            "Epoch 167/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5689 - accuracy: 0.8140 - val_loss: 0.5302 - val_accuracy: 0.8287\n",
            "Epoch 168/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5691 - accuracy: 0.8138 - val_loss: 0.5527 - val_accuracy: 0.8240\n",
            "Epoch 169/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5687 - accuracy: 0.8120 - val_loss: 0.5439 - val_accuracy: 0.8295\n",
            "Epoch 170/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5619 - accuracy: 0.8167 - val_loss: 0.5131 - val_accuracy: 0.8353\n",
            "Epoch 171/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5650 - accuracy: 0.8157 - val_loss: 0.5465 - val_accuracy: 0.8292\n",
            "Epoch 172/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5601 - accuracy: 0.8166 - val_loss: 0.5617 - val_accuracy: 0.8224\n",
            "Epoch 173/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5599 - accuracy: 0.8170 - val_loss: 0.5190 - val_accuracy: 0.8343\n",
            "Epoch 174/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5582 - accuracy: 0.8179 - val_loss: 0.5217 - val_accuracy: 0.8329\n",
            "Epoch 175/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5566 - accuracy: 0.8195 - val_loss: 0.5113 - val_accuracy: 0.8355\n",
            "Epoch 176/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5549 - accuracy: 0.8202 - val_loss: 0.5135 - val_accuracy: 0.8356\n",
            "Epoch 177/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5503 - accuracy: 0.8208 - val_loss: 0.5280 - val_accuracy: 0.8349\n",
            "Epoch 178/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5521 - accuracy: 0.8210 - val_loss: 0.5127 - val_accuracy: 0.8368\n",
            "Epoch 179/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5528 - accuracy: 0.8210 - val_loss: 0.5014 - val_accuracy: 0.8420\n",
            "Epoch 180/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5535 - accuracy: 0.8214 - val_loss: 0.5240 - val_accuracy: 0.8361\n",
            "Epoch 181/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5482 - accuracy: 0.8218 - val_loss: 0.4991 - val_accuracy: 0.8403\n",
            "Epoch 182/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.5509 - accuracy: 0.8201 - val_loss: 0.5030 - val_accuracy: 0.8391\n",
            "Epoch 183/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5465 - accuracy: 0.8216 - val_loss: 0.5245 - val_accuracy: 0.8348\n",
            "Epoch 184/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5475 - accuracy: 0.8204 - val_loss: 0.5087 - val_accuracy: 0.8387\n",
            "Epoch 185/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.5445 - accuracy: 0.8235 - val_loss: 0.5352 - val_accuracy: 0.8316\n",
            "Epoch 186/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.5403 - accuracy: 0.8244 - val_loss: 0.5191 - val_accuracy: 0.8371\n",
            "Epoch 187/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5397 - accuracy: 0.8244 - val_loss: 0.5469 - val_accuracy: 0.8276\n",
            "Epoch 188/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.5477 - accuracy: 0.8219 - val_loss: 0.5017 - val_accuracy: 0.8401\n",
            "Epoch 189/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5385 - accuracy: 0.8238 - val_loss: 0.4974 - val_accuracy: 0.8417\n",
            "Epoch 190/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5397 - accuracy: 0.8231 - val_loss: 0.5089 - val_accuracy: 0.8359\n",
            "Epoch 191/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5366 - accuracy: 0.8256 - val_loss: 0.5257 - val_accuracy: 0.8343\n",
            "Epoch 192/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5308 - accuracy: 0.8251 - val_loss: 0.4986 - val_accuracy: 0.8367\n",
            "Epoch 193/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5309 - accuracy: 0.8259 - val_loss: 0.5216 - val_accuracy: 0.8353\n",
            "Epoch 194/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5295 - accuracy: 0.8272 - val_loss: 0.5332 - val_accuracy: 0.8304\n",
            "Epoch 195/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5347 - accuracy: 0.8263 - val_loss: 0.5319 - val_accuracy: 0.8296\n",
            "Epoch 196/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.5244 - accuracy: 0.8274 - val_loss: 0.5129 - val_accuracy: 0.8371\n",
            "Epoch 197/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5303 - accuracy: 0.8248 - val_loss: 0.5018 - val_accuracy: 0.8416\n",
            "Epoch 198/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5260 - accuracy: 0.8288 - val_loss: 0.4975 - val_accuracy: 0.8419\n",
            "Epoch 199/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5253 - accuracy: 0.8285 - val_loss: 0.5104 - val_accuracy: 0.8367\n",
            "Epoch 200/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5281 - accuracy: 0.8291 - val_loss: 0.5277 - val_accuracy: 0.8327\n",
            "Epoch 201/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5278 - accuracy: 0.8285 - val_loss: 0.5111 - val_accuracy: 0.8395\n",
            "Epoch 202/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5228 - accuracy: 0.8313 - val_loss: 0.5003 - val_accuracy: 0.8416\n",
            "Epoch 203/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5262 - accuracy: 0.8276 - val_loss: 0.5178 - val_accuracy: 0.8375\n",
            "Epoch 204/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5167 - accuracy: 0.8304 - val_loss: 0.4955 - val_accuracy: 0.8421\n",
            "Epoch 205/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5171 - accuracy: 0.8317 - val_loss: 0.4959 - val_accuracy: 0.8433\n",
            "Epoch 206/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5162 - accuracy: 0.8318 - val_loss: 0.4874 - val_accuracy: 0.8465\n",
            "Epoch 207/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5191 - accuracy: 0.8305 - val_loss: 0.4951 - val_accuracy: 0.8435\n",
            "Epoch 208/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5163 - accuracy: 0.8307 - val_loss: 0.5055 - val_accuracy: 0.8405\n",
            "Epoch 209/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5171 - accuracy: 0.8303 - val_loss: 0.4740 - val_accuracy: 0.8495\n",
            "Epoch 210/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5150 - accuracy: 0.8326 - val_loss: 0.4810 - val_accuracy: 0.8471\n",
            "Epoch 211/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5124 - accuracy: 0.8328 - val_loss: 0.5071 - val_accuracy: 0.8419\n",
            "Epoch 212/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5115 - accuracy: 0.8339 - val_loss: 0.5123 - val_accuracy: 0.8405\n",
            "Epoch 213/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5136 - accuracy: 0.8322 - val_loss: 0.4901 - val_accuracy: 0.8453\n",
            "Epoch 214/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.5087 - accuracy: 0.8331 - val_loss: 0.4749 - val_accuracy: 0.8512\n",
            "Epoch 215/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5094 - accuracy: 0.8331 - val_loss: 0.5300 - val_accuracy: 0.8355\n",
            "Epoch 216/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5075 - accuracy: 0.8327 - val_loss: 0.4972 - val_accuracy: 0.8425\n",
            "Epoch 217/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5034 - accuracy: 0.8345 - val_loss: 0.4979 - val_accuracy: 0.8433\n",
            "Epoch 218/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5066 - accuracy: 0.8337 - val_loss: 0.5047 - val_accuracy: 0.8417\n",
            "Epoch 219/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4995 - accuracy: 0.8380 - val_loss: 0.4933 - val_accuracy: 0.8424\n",
            "Epoch 220/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5014 - accuracy: 0.8367 - val_loss: 0.4669 - val_accuracy: 0.8521\n",
            "Epoch 221/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5037 - accuracy: 0.8365 - val_loss: 0.5109 - val_accuracy: 0.8421\n",
            "Epoch 222/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4960 - accuracy: 0.8369 - val_loss: 0.4908 - val_accuracy: 0.8459\n",
            "Epoch 223/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5015 - accuracy: 0.8363 - val_loss: 0.5239 - val_accuracy: 0.8375\n",
            "Epoch 224/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4957 - accuracy: 0.8380 - val_loss: 0.4975 - val_accuracy: 0.8448\n",
            "Epoch 225/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4992 - accuracy: 0.8368 - val_loss: 0.4796 - val_accuracy: 0.8493\n",
            "Epoch 226/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4971 - accuracy: 0.8378 - val_loss: 0.5472 - val_accuracy: 0.8292\n",
            "Epoch 227/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.4975 - accuracy: 0.8380 - val_loss: 0.4648 - val_accuracy: 0.8528\n",
            "Epoch 228/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4942 - accuracy: 0.8392 - val_loss: 0.4725 - val_accuracy: 0.8507\n",
            "Epoch 229/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4921 - accuracy: 0.8375 - val_loss: 0.5014 - val_accuracy: 0.8444\n",
            "Epoch 230/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4867 - accuracy: 0.8406 - val_loss: 0.5005 - val_accuracy: 0.8437\n",
            "Epoch 231/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4904 - accuracy: 0.8412 - val_loss: 0.5137 - val_accuracy: 0.8401\n",
            "Epoch 232/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4917 - accuracy: 0.8385 - val_loss: 0.4731 - val_accuracy: 0.8513\n",
            "Epoch 233/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4887 - accuracy: 0.8402 - val_loss: 0.4885 - val_accuracy: 0.8459\n",
            "Epoch 234/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4911 - accuracy: 0.8402 - val_loss: 0.4803 - val_accuracy: 0.8497\n",
            "Epoch 235/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4947 - accuracy: 0.8393 - val_loss: 0.4684 - val_accuracy: 0.8536\n",
            "Epoch 236/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4818 - accuracy: 0.8418 - val_loss: 0.4692 - val_accuracy: 0.8536\n",
            "Epoch 237/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.4821 - accuracy: 0.8426 - val_loss: 0.4932 - val_accuracy: 0.8460\n",
            "Epoch 238/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4861 - accuracy: 0.8408 - val_loss: 0.4935 - val_accuracy: 0.8460\n",
            "Epoch 239/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4857 - accuracy: 0.8408 - val_loss: 0.4929 - val_accuracy: 0.8432\n",
            "Epoch 240/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4843 - accuracy: 0.8421 - val_loss: 0.5290 - val_accuracy: 0.8369\n",
            "Epoch 241/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4832 - accuracy: 0.8441 - val_loss: 0.5025 - val_accuracy: 0.8437\n",
            "Epoch 242/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4904 - accuracy: 0.8382 - val_loss: 0.4738 - val_accuracy: 0.8528\n",
            "Epoch 243/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4900 - accuracy: 0.8403 - val_loss: 0.4839 - val_accuracy: 0.8483\n",
            "Epoch 244/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4840 - accuracy: 0.8393 - val_loss: 0.4709 - val_accuracy: 0.8516\n",
            "Epoch 245/250\n",
            "703/703 [==============================] - 28s 40ms/step - loss: 0.4786 - accuracy: 0.8428 - val_loss: 0.4782 - val_accuracy: 0.8533\n",
            "Epoch 246/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.4814 - accuracy: 0.8429 - val_loss: 0.5354 - val_accuracy: 0.8373\n",
            "Epoch 247/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4784 - accuracy: 0.8424 - val_loss: 0.4925 - val_accuracy: 0.8487\n",
            "Epoch 248/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4824 - accuracy: 0.8413 - val_loss: 0.4783 - val_accuracy: 0.8535\n",
            "Epoch 249/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4755 - accuracy: 0.8450 - val_loss: 0.4522 - val_accuracy: 0.8565\n",
            "Epoch 250/250\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 0.4777 - accuracy: 0.8448 - val_loss: 0.4786 - val_accuracy: 0.8497\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 226.3452 - accuracy: 0.1359\n",
            "> 13.587\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_pixels2(testX):\n",
        "  # convert from integers to floats\n",
        "  testX_norm = testX.astype('float32')\n",
        "  # normalize to range 0-1\n",
        "  testX_norm = testX_norm / 255.0\n",
        "  \n",
        "  # return normalized images\n",
        "  return testX_norm"
      ],
      "metadata": {
        "id": "OwcjwLRtiYKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc = model.evaluate(prep_pixels2(e),f, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MO6nMI7hjFV",
        "outputId": "74d8b1e3-b6df-47de-f80e-9e8b9582eada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "704/704 [==============================] - 3s 4ms/step - loss: 0.4755 - accuracy: 0.8492\n"
          ]
        }
      ]
    }
  ]
}

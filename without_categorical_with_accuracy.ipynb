{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2022Group3/projectNotebooks/blob/main/without_categorical_with_accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glekdw9QYM7R"
      },
      "outputs": [],
      "source": [
        "# baseline model with dropout and data augmentation on the cifar10 dataset\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_WXoAU7YYi-",
        "outputId": "72fb511a-e415-488c-bb68-8c67012d360c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "  data=np.load(r'drive/MyDrive/data_modified.npz')\n",
        "  data=dict(zip((\"{}\".format(k) for k in data),(data[k] for k in data)))\n",
        "  trainX=data['train']\n",
        "  trainy=data['ytrain']\n",
        "  validationX=data['validation']\n",
        "  validationy=data['yvalidation']\n",
        "  testX=data['test']\n",
        "  testy=data['ytest']\n",
        "  return trainX,trainy,validationX,validationy,testX,testy"
      ],
      "metadata": {
        "id": "VX6_KJutYj_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d,e,f=load_dataset()"
      ],
      "metadata": {
        "id": "p3JQ7hjLfE8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.shape\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXwFlEPMgz5n",
        "outputId": "78d618c6-bfaa-440c-c826-87c2c8906696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 0, 9, ..., 7, 8, 4], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxhQd_sUfMsF",
        "outputId": "a60556ea-5c18-495a-f18b-b75850b3d9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45000"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hZo69ukfQjH",
        "outputId": "18df80e1-2ddb-45a9-8804-5018e3df571b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scale pixels\n",
        "def prep_pixels(train,validation):\n",
        "  # convert from integers to floats\n",
        "  train_norm = train.astype('float32')\n",
        "  validation_norm = validation.astype('float32')\n",
        "  # normalize to range 0-1\n",
        "  train_norm = train_norm / 255.0\n",
        "  validation_norm = validation / 255.0\n",
        "  # return normalized images\n",
        "  return train_norm, validation_norm"
      ],
      "metadata": {
        "id": "hotdCojsYpI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # list all data in history\n",
        "    print(history.history.keys())"
      ],
      "metadata": {
        "id": "XJGrRoVsYqyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cnn model\n",
        "def define_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(28, activation='softmax'))\n",
        "\t# compile model\n",
        "  opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "  # model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  # model.compile(optimizer=opt,loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "PbaYP_yKYvZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='validation')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='validation')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\n"
      ],
      "metadata": {
        "id": "h5sKaXpiY07G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "  trainX, trainy,validationX,validationy, testX, testy = load_dataset()\n",
        "\t# prepare pixel data\n",
        "  trainX, validationX = prep_pixels(trainX, validationX)\n",
        "\t# define model\n",
        "  model = define_model()\n",
        "\t# create data generator\n",
        "  datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "\t# prepare iterator\n",
        "  it_train = datagen.flow(trainX, trainy, batch_size=64)\n",
        "\t# fit model\n",
        "  steps = int(trainX.shape[0] / 64)\n",
        "  history = model.fit(it_train, steps_per_epoch=steps, epochs=250, validation_data=(validationX, validationy), verbose=1)\n",
        "\t# evaluate model\n",
        "  _, acc = model.evaluate(testX, testy, verbose=1)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "  summarize_diagnostics(history)\n",
        "\t#save model\n",
        "  model.save('/content/drive/MyDrive/model2.h5')\n",
        "  return model"
      ],
      "metadata": {
        "id": "hjm8k2rwY6rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entry point, run the test harness\n",
        "model=run_test_harness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dSBBUTivY8kH",
        "outputId": "97c7e588-3b7a-4a8f-c962-80c351ff4e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "703/703 [==============================] - 36s 34ms/step - loss: 2.8622 - accuracy: 0.2416 - val_loss: 1.9587 - val_accuracy: 0.3825\n",
            "Epoch 2/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 2.1599 - accuracy: 0.3445 - val_loss: 1.8186 - val_accuracy: 0.4208\n",
            "Epoch 3/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 1.9485 - accuracy: 0.3873 - val_loss: 1.6977 - val_accuracy: 0.4405\n",
            "Epoch 4/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.8107 - accuracy: 0.4155 - val_loss: 1.7268 - val_accuracy: 0.4511\n",
            "Epoch 5/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.7314 - accuracy: 0.4392 - val_loss: 1.6069 - val_accuracy: 0.4760\n",
            "Epoch 6/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 1.6588 - accuracy: 0.4560 - val_loss: 1.5534 - val_accuracy: 0.5013\n",
            "Epoch 7/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 1.5940 - accuracy: 0.4735 - val_loss: 1.5538 - val_accuracy: 0.4944\n",
            "Epoch 8/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.5448 - accuracy: 0.4906 - val_loss: 1.4619 - val_accuracy: 0.5279\n",
            "Epoch 9/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.4980 - accuracy: 0.5066 - val_loss: 1.6223 - val_accuracy: 0.4785\n",
            "Epoch 10/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.4587 - accuracy: 0.5204 - val_loss: 1.5076 - val_accuracy: 0.5120\n",
            "Epoch 11/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.4293 - accuracy: 0.5274 - val_loss: 1.4832 - val_accuracy: 0.5231\n",
            "Epoch 12/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.3968 - accuracy: 0.5394 - val_loss: 1.3922 - val_accuracy: 0.5453\n",
            "Epoch 13/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 1.3693 - accuracy: 0.5496 - val_loss: 1.5129 - val_accuracy: 0.5079\n",
            "Epoch 14/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.3487 - accuracy: 0.5560 - val_loss: 1.3855 - val_accuracy: 0.5507\n",
            "Epoch 15/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.3204 - accuracy: 0.5664 - val_loss: 1.2767 - val_accuracy: 0.5840\n",
            "Epoch 16/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.2936 - accuracy: 0.5715 - val_loss: 1.2476 - val_accuracy: 0.5911\n",
            "Epoch 17/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 1.2735 - accuracy: 0.5804 - val_loss: 1.2252 - val_accuracy: 0.5937\n",
            "Epoch 18/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2570 - accuracy: 0.5866 - val_loss: 1.2909 - val_accuracy: 0.5737\n",
            "Epoch 19/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2385 - accuracy: 0.5923 - val_loss: 1.3503 - val_accuracy: 0.5723\n",
            "Epoch 20/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2224 - accuracy: 0.6000 - val_loss: 1.1300 - val_accuracy: 0.6267\n",
            "Epoch 21/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.2062 - accuracy: 0.6035 - val_loss: 1.1055 - val_accuracy: 0.6359\n",
            "Epoch 22/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.1853 - accuracy: 0.6092 - val_loss: 1.3103 - val_accuracy: 0.5779\n",
            "Epoch 23/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.1709 - accuracy: 0.6154 - val_loss: 1.1736 - val_accuracy: 0.6220\n",
            "Epoch 24/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 1.1528 - accuracy: 0.6234 - val_loss: 1.2342 - val_accuracy: 0.5960\n",
            "Epoch 25/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.1433 - accuracy: 0.6226 - val_loss: 1.1115 - val_accuracy: 0.6396\n",
            "Epoch 26/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.1325 - accuracy: 0.6288 - val_loss: 1.1501 - val_accuracy: 0.6212\n",
            "Epoch 27/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.1160 - accuracy: 0.6313 - val_loss: 1.1295 - val_accuracy: 0.6357\n",
            "Epoch 28/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.1062 - accuracy: 0.6352 - val_loss: 1.0206 - val_accuracy: 0.6653\n",
            "Epoch 29/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0932 - accuracy: 0.6422 - val_loss: 1.0279 - val_accuracy: 0.6609\n",
            "Epoch 30/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 1.0827 - accuracy: 0.6464 - val_loss: 0.9928 - val_accuracy: 0.6741\n",
            "Epoch 31/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.0712 - accuracy: 0.6504 - val_loss: 1.0522 - val_accuracy: 0.6620\n",
            "Epoch 32/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 1.0587 - accuracy: 0.6533 - val_loss: 0.9975 - val_accuracy: 0.6733\n",
            "Epoch 33/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0506 - accuracy: 0.6540 - val_loss: 0.9638 - val_accuracy: 0.6825\n",
            "Epoch 34/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 1.0354 - accuracy: 0.6586 - val_loss: 1.0005 - val_accuracy: 0.6731\n",
            "Epoch 35/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0297 - accuracy: 0.6621 - val_loss: 0.9554 - val_accuracy: 0.6825\n",
            "Epoch 36/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 1.0256 - accuracy: 0.6658 - val_loss: 0.8949 - val_accuracy: 0.7043\n",
            "Epoch 37/250\n",
            "703/703 [==============================] - 23s 32ms/step - loss: 1.0171 - accuracy: 0.6663 - val_loss: 0.9488 - val_accuracy: 0.6895\n",
            "Epoch 38/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 1.0086 - accuracy: 0.6691 - val_loss: 1.0400 - val_accuracy: 0.6701\n",
            "Epoch 39/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9883 - accuracy: 0.6742 - val_loss: 0.9031 - val_accuracy: 0.7035\n",
            "Epoch 40/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.9861 - accuracy: 0.6771 - val_loss: 0.9135 - val_accuracy: 0.6944\n",
            "Epoch 41/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.9814 - accuracy: 0.6791 - val_loss: 0.8709 - val_accuracy: 0.7144\n",
            "Epoch 42/250\n",
            "703/703 [==============================] - 23s 32ms/step - loss: 0.9696 - accuracy: 0.6811 - val_loss: 0.9410 - val_accuracy: 0.6905\n",
            "Epoch 43/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9642 - accuracy: 0.6835 - val_loss: 0.8891 - val_accuracy: 0.7055\n",
            "Epoch 44/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.9537 - accuracy: 0.6900 - val_loss: 0.8690 - val_accuracy: 0.7101\n",
            "Epoch 45/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9480 - accuracy: 0.6887 - val_loss: 0.8780 - val_accuracy: 0.7077\n",
            "Epoch 46/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9376 - accuracy: 0.6935 - val_loss: 0.8488 - val_accuracy: 0.7176\n",
            "Epoch 47/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9345 - accuracy: 0.6934 - val_loss: 0.9038 - val_accuracy: 0.7027\n",
            "Epoch 48/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.9219 - accuracy: 0.6971 - val_loss: 0.8398 - val_accuracy: 0.7240\n",
            "Epoch 49/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9153 - accuracy: 0.7015 - val_loss: 0.8583 - val_accuracy: 0.7201\n",
            "Epoch 50/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.9135 - accuracy: 0.7027 - val_loss: 0.8176 - val_accuracy: 0.7309\n",
            "Epoch 51/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.9068 - accuracy: 0.7037 - val_loss: 0.8632 - val_accuracy: 0.7164\n",
            "Epoch 52/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.9044 - accuracy: 0.7062 - val_loss: 0.8801 - val_accuracy: 0.7136\n",
            "Epoch 53/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8905 - accuracy: 0.7099 - val_loss: 0.8112 - val_accuracy: 0.7315\n",
            "Epoch 54/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8941 - accuracy: 0.7102 - val_loss: 0.8133 - val_accuracy: 0.7324\n",
            "Epoch 55/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8796 - accuracy: 0.7135 - val_loss: 0.8496 - val_accuracy: 0.7245\n",
            "Epoch 56/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8710 - accuracy: 0.7153 - val_loss: 0.8483 - val_accuracy: 0.7213\n",
            "Epoch 57/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8745 - accuracy: 0.7127 - val_loss: 0.8090 - val_accuracy: 0.7345\n",
            "Epoch 58/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8663 - accuracy: 0.7162 - val_loss: 0.8627 - val_accuracy: 0.7187\n",
            "Epoch 59/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8567 - accuracy: 0.7207 - val_loss: 0.8076 - val_accuracy: 0.7343\n",
            "Epoch 60/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8576 - accuracy: 0.7219 - val_loss: 0.8332 - val_accuracy: 0.7260\n",
            "Epoch 61/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8455 - accuracy: 0.7239 - val_loss: 0.7594 - val_accuracy: 0.7481\n",
            "Epoch 62/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8405 - accuracy: 0.7271 - val_loss: 0.7509 - val_accuracy: 0.7544\n",
            "Epoch 63/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.8386 - accuracy: 0.7266 - val_loss: 0.7473 - val_accuracy: 0.7553\n",
            "Epoch 64/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8323 - accuracy: 0.7282 - val_loss: 0.7837 - val_accuracy: 0.7437\n",
            "Epoch 65/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8327 - accuracy: 0.7279 - val_loss: 0.7763 - val_accuracy: 0.7428\n",
            "Epoch 66/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.8221 - accuracy: 0.7309 - val_loss: 0.7073 - val_accuracy: 0.7664\n",
            "Epoch 67/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.8178 - accuracy: 0.7338 - val_loss: 0.7831 - val_accuracy: 0.7476\n",
            "Epoch 68/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8119 - accuracy: 0.7353 - val_loss: 0.7869 - val_accuracy: 0.7463\n",
            "Epoch 69/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.8138 - accuracy: 0.7364 - val_loss: 0.7725 - val_accuracy: 0.7483\n",
            "Epoch 70/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.8076 - accuracy: 0.7360 - val_loss: 0.7423 - val_accuracy: 0.7565\n",
            "Epoch 71/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7981 - accuracy: 0.7409 - val_loss: 0.7180 - val_accuracy: 0.7644\n",
            "Epoch 72/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.7946 - accuracy: 0.7411 - val_loss: 0.7445 - val_accuracy: 0.7559\n",
            "Epoch 73/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7900 - accuracy: 0.7432 - val_loss: 0.7001 - val_accuracy: 0.7696\n",
            "Epoch 74/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7883 - accuracy: 0.7417 - val_loss: 0.6770 - val_accuracy: 0.7797\n",
            "Epoch 75/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7830 - accuracy: 0.7438 - val_loss: 0.7193 - val_accuracy: 0.7668\n",
            "Epoch 76/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7766 - accuracy: 0.7462 - val_loss: 0.7332 - val_accuracy: 0.7635\n",
            "Epoch 77/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7753 - accuracy: 0.7480 - val_loss: 0.7709 - val_accuracy: 0.7523\n",
            "Epoch 78/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7705 - accuracy: 0.7516 - val_loss: 0.7521 - val_accuracy: 0.7575\n",
            "Epoch 79/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.7718 - accuracy: 0.7487 - val_loss: 0.6751 - val_accuracy: 0.7751\n",
            "Epoch 80/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7657 - accuracy: 0.7529 - val_loss: 0.7356 - val_accuracy: 0.7613\n",
            "Epoch 81/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7614 - accuracy: 0.7522 - val_loss: 0.6768 - val_accuracy: 0.7767\n",
            "Epoch 82/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7591 - accuracy: 0.7514 - val_loss: 0.6843 - val_accuracy: 0.7789\n",
            "Epoch 83/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.7564 - accuracy: 0.7557 - val_loss: 0.6892 - val_accuracy: 0.7805\n",
            "Epoch 84/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7546 - accuracy: 0.7543 - val_loss: 0.7213 - val_accuracy: 0.7644\n",
            "Epoch 85/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7447 - accuracy: 0.7557 - val_loss: 0.6436 - val_accuracy: 0.7915\n",
            "Epoch 86/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7423 - accuracy: 0.7585 - val_loss: 0.6560 - val_accuracy: 0.7856\n",
            "Epoch 87/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.7367 - accuracy: 0.7615 - val_loss: 0.6696 - val_accuracy: 0.7816\n",
            "Epoch 88/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7408 - accuracy: 0.7601 - val_loss: 0.7020 - val_accuracy: 0.7699\n",
            "Epoch 89/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7356 - accuracy: 0.7602 - val_loss: 0.6510 - val_accuracy: 0.7903\n",
            "Epoch 90/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7315 - accuracy: 0.7609 - val_loss: 0.6547 - val_accuracy: 0.7897\n",
            "Epoch 91/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.7306 - accuracy: 0.7613 - val_loss: 0.6281 - val_accuracy: 0.7939\n",
            "Epoch 92/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7208 - accuracy: 0.7656 - val_loss: 0.6224 - val_accuracy: 0.7987\n",
            "Epoch 93/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7273 - accuracy: 0.7652 - val_loss: 0.6522 - val_accuracy: 0.7899\n",
            "Epoch 94/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7177 - accuracy: 0.7649 - val_loss: 0.6828 - val_accuracy: 0.7792\n",
            "Epoch 95/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7190 - accuracy: 0.7686 - val_loss: 0.6493 - val_accuracy: 0.7877\n",
            "Epoch 96/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.7115 - accuracy: 0.7685 - val_loss: 0.6325 - val_accuracy: 0.7947\n",
            "Epoch 97/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.7104 - accuracy: 0.7690 - val_loss: 0.6280 - val_accuracy: 0.7967\n",
            "Epoch 98/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7023 - accuracy: 0.7703 - val_loss: 0.6460 - val_accuracy: 0.7905\n",
            "Epoch 99/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.7087 - accuracy: 0.7700 - val_loss: 0.6347 - val_accuracy: 0.7980\n",
            "Epoch 100/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.7015 - accuracy: 0.7715 - val_loss: 0.6737 - val_accuracy: 0.7840\n",
            "Epoch 101/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6984 - accuracy: 0.7748 - val_loss: 0.6664 - val_accuracy: 0.7893\n",
            "Epoch 102/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6910 - accuracy: 0.7756 - val_loss: 0.6661 - val_accuracy: 0.7868\n",
            "Epoch 103/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6935 - accuracy: 0.7748 - val_loss: 0.6283 - val_accuracy: 0.7983\n",
            "Epoch 104/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6864 - accuracy: 0.7780 - val_loss: 0.6498 - val_accuracy: 0.7917\n",
            "Epoch 105/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6869 - accuracy: 0.7742 - val_loss: 0.6139 - val_accuracy: 0.8007\n",
            "Epoch 106/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6845 - accuracy: 0.7784 - val_loss: 0.6894 - val_accuracy: 0.7792\n",
            "Epoch 107/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6803 - accuracy: 0.7782 - val_loss: 0.6036 - val_accuracy: 0.8056\n",
            "Epoch 108/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6732 - accuracy: 0.7814 - val_loss: 0.5932 - val_accuracy: 0.8092\n",
            "Epoch 109/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6770 - accuracy: 0.7794 - val_loss: 0.5872 - val_accuracy: 0.8112\n",
            "Epoch 110/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6800 - accuracy: 0.7807 - val_loss: 0.6449 - val_accuracy: 0.7947\n",
            "Epoch 111/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.6709 - accuracy: 0.7827 - val_loss: 0.6280 - val_accuracy: 0.8009\n",
            "Epoch 112/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.6730 - accuracy: 0.7812 - val_loss: 0.6281 - val_accuracy: 0.7981\n",
            "Epoch 113/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.6690 - accuracy: 0.7822 - val_loss: 0.6171 - val_accuracy: 0.8036\n",
            "Epoch 114/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6638 - accuracy: 0.7856 - val_loss: 0.5621 - val_accuracy: 0.8195\n",
            "Epoch 115/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6588 - accuracy: 0.7844 - val_loss: 0.5895 - val_accuracy: 0.8084\n",
            "Epoch 116/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6522 - accuracy: 0.7877 - val_loss: 0.6372 - val_accuracy: 0.7957\n",
            "Epoch 117/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6647 - accuracy: 0.7844 - val_loss: 0.5781 - val_accuracy: 0.8165\n",
            "Epoch 118/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6553 - accuracy: 0.7862 - val_loss: 0.5992 - val_accuracy: 0.8067\n",
            "Epoch 119/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6553 - accuracy: 0.7885 - val_loss: 0.6047 - val_accuracy: 0.8048\n",
            "Epoch 120/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6457 - accuracy: 0.7905 - val_loss: 0.6409 - val_accuracy: 0.7948\n",
            "Epoch 121/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6476 - accuracy: 0.7882 - val_loss: 0.6250 - val_accuracy: 0.7996\n",
            "Epoch 122/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6540 - accuracy: 0.7870 - val_loss: 0.6138 - val_accuracy: 0.8000\n",
            "Epoch 123/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6482 - accuracy: 0.7896 - val_loss: 0.5713 - val_accuracy: 0.8165\n",
            "Epoch 124/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6427 - accuracy: 0.7911 - val_loss: 0.5826 - val_accuracy: 0.8129\n",
            "Epoch 125/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6398 - accuracy: 0.7909 - val_loss: 0.5735 - val_accuracy: 0.8168\n",
            "Epoch 126/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6388 - accuracy: 0.7930 - val_loss: 0.6208 - val_accuracy: 0.8023\n",
            "Epoch 127/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6416 - accuracy: 0.7909 - val_loss: 0.5779 - val_accuracy: 0.8141\n",
            "Epoch 128/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6414 - accuracy: 0.7923 - val_loss: 0.5640 - val_accuracy: 0.8175\n",
            "Epoch 129/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6339 - accuracy: 0.7936 - val_loss: 0.6022 - val_accuracy: 0.8077\n",
            "Epoch 130/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6326 - accuracy: 0.7944 - val_loss: 0.6040 - val_accuracy: 0.8065\n",
            "Epoch 131/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.6265 - accuracy: 0.7963 - val_loss: 0.5596 - val_accuracy: 0.8228\n",
            "Epoch 132/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6250 - accuracy: 0.7983 - val_loss: 0.5858 - val_accuracy: 0.8116\n",
            "Epoch 133/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6309 - accuracy: 0.7950 - val_loss: 0.5786 - val_accuracy: 0.8168\n",
            "Epoch 134/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6263 - accuracy: 0.7960 - val_loss: 0.5487 - val_accuracy: 0.8233\n",
            "Epoch 135/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.6180 - accuracy: 0.7999 - val_loss: 0.5675 - val_accuracy: 0.8196\n",
            "Epoch 136/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.6171 - accuracy: 0.7979 - val_loss: 0.6051 - val_accuracy: 0.8061\n",
            "Epoch 137/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.6125 - accuracy: 0.8023 - val_loss: 0.6065 - val_accuracy: 0.8083\n",
            "Epoch 138/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.6123 - accuracy: 0.8028 - val_loss: 0.6218 - val_accuracy: 0.7999\n",
            "Epoch 139/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6171 - accuracy: 0.8002 - val_loss: 0.5424 - val_accuracy: 0.8284\n",
            "Epoch 140/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.6075 - accuracy: 0.8033 - val_loss: 0.5607 - val_accuracy: 0.8239\n",
            "Epoch 141/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6085 - accuracy: 0.8005 - val_loss: 0.5599 - val_accuracy: 0.8195\n",
            "Epoch 142/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6042 - accuracy: 0.8042 - val_loss: 0.5925 - val_accuracy: 0.8123\n",
            "Epoch 143/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.6069 - accuracy: 0.8033 - val_loss: 0.5567 - val_accuracy: 0.8215\n",
            "Epoch 144/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.6064 - accuracy: 0.8033 - val_loss: 0.5814 - val_accuracy: 0.8181\n",
            "Epoch 145/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.6121 - accuracy: 0.8010 - val_loss: 0.5442 - val_accuracy: 0.8217\n",
            "Epoch 146/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5956 - accuracy: 0.8075 - val_loss: 0.5872 - val_accuracy: 0.8148\n",
            "Epoch 147/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5963 - accuracy: 0.8060 - val_loss: 0.5849 - val_accuracy: 0.8125\n",
            "Epoch 148/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5960 - accuracy: 0.8066 - val_loss: 0.5894 - val_accuracy: 0.8129\n",
            "Epoch 149/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5915 - accuracy: 0.8069 - val_loss: 0.5664 - val_accuracy: 0.8185\n",
            "Epoch 150/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5905 - accuracy: 0.8089 - val_loss: 0.5520 - val_accuracy: 0.8212\n",
            "Epoch 151/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5916 - accuracy: 0.8060 - val_loss: 0.5756 - val_accuracy: 0.8188\n",
            "Epoch 152/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5926 - accuracy: 0.8064 - val_loss: 0.5327 - val_accuracy: 0.8275\n",
            "Epoch 153/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5875 - accuracy: 0.8091 - val_loss: 0.5662 - val_accuracy: 0.8196\n",
            "Epoch 154/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5885 - accuracy: 0.8096 - val_loss: 0.5437 - val_accuracy: 0.8261\n",
            "Epoch 155/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5845 - accuracy: 0.8100 - val_loss: 0.5190 - val_accuracy: 0.8320\n",
            "Epoch 156/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5858 - accuracy: 0.8092 - val_loss: 0.5682 - val_accuracy: 0.8204\n",
            "Epoch 157/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5858 - accuracy: 0.8089 - val_loss: 0.5218 - val_accuracy: 0.8328\n",
            "Epoch 158/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5767 - accuracy: 0.8143 - val_loss: 0.5597 - val_accuracy: 0.8212\n",
            "Epoch 159/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5835 - accuracy: 0.8104 - val_loss: 0.5484 - val_accuracy: 0.8211\n",
            "Epoch 160/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5764 - accuracy: 0.8115 - val_loss: 0.5571 - val_accuracy: 0.8227\n",
            "Epoch 161/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5819 - accuracy: 0.8111 - val_loss: 0.5493 - val_accuracy: 0.8223\n",
            "Epoch 162/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5753 - accuracy: 0.8120 - val_loss: 0.5453 - val_accuracy: 0.8251\n",
            "Epoch 163/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.5741 - accuracy: 0.8119 - val_loss: 0.5348 - val_accuracy: 0.8271\n",
            "Epoch 164/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5722 - accuracy: 0.8129 - val_loss: 0.5192 - val_accuracy: 0.8323\n",
            "Epoch 165/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5719 - accuracy: 0.8154 - val_loss: 0.5329 - val_accuracy: 0.8284\n",
            "Epoch 166/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5719 - accuracy: 0.8125 - val_loss: 0.5524 - val_accuracy: 0.8236\n",
            "Epoch 167/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5689 - accuracy: 0.8140 - val_loss: 0.5302 - val_accuracy: 0.8287\n",
            "Epoch 168/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5691 - accuracy: 0.8138 - val_loss: 0.5527 - val_accuracy: 0.8240\n",
            "Epoch 169/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5687 - accuracy: 0.8120 - val_loss: 0.5439 - val_accuracy: 0.8295\n",
            "Epoch 170/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5619 - accuracy: 0.8167 - val_loss: 0.5131 - val_accuracy: 0.8353\n",
            "Epoch 171/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5650 - accuracy: 0.8157 - val_loss: 0.5465 - val_accuracy: 0.8292\n",
            "Epoch 172/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5601 - accuracy: 0.8166 - val_loss: 0.5617 - val_accuracy: 0.8224\n",
            "Epoch 173/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5599 - accuracy: 0.8170 - val_loss: 0.5190 - val_accuracy: 0.8343\n",
            "Epoch 174/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5582 - accuracy: 0.8179 - val_loss: 0.5217 - val_accuracy: 0.8329\n",
            "Epoch 175/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5566 - accuracy: 0.8195 - val_loss: 0.5113 - val_accuracy: 0.8355\n",
            "Epoch 176/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5549 - accuracy: 0.8202 - val_loss: 0.5135 - val_accuracy: 0.8356\n",
            "Epoch 177/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5503 - accuracy: 0.8208 - val_loss: 0.5280 - val_accuracy: 0.8349\n",
            "Epoch 178/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5521 - accuracy: 0.8210 - val_loss: 0.5127 - val_accuracy: 0.8368\n",
            "Epoch 179/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5528 - accuracy: 0.8210 - val_loss: 0.5014 - val_accuracy: 0.8420\n",
            "Epoch 180/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5535 - accuracy: 0.8214 - val_loss: 0.5240 - val_accuracy: 0.8361\n",
            "Epoch 181/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5482 - accuracy: 0.8218 - val_loss: 0.4991 - val_accuracy: 0.8403\n",
            "Epoch 182/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.5509 - accuracy: 0.8201 - val_loss: 0.5030 - val_accuracy: 0.8391\n",
            "Epoch 183/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5465 - accuracy: 0.8216 - val_loss: 0.5245 - val_accuracy: 0.8348\n",
            "Epoch 184/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5475 - accuracy: 0.8204 - val_loss: 0.5087 - val_accuracy: 0.8387\n",
            "Epoch 185/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.5445 - accuracy: 0.8235 - val_loss: 0.5352 - val_accuracy: 0.8316\n",
            "Epoch 186/250\n",
            "703/703 [==============================] - 26s 36ms/step - loss: 0.5403 - accuracy: 0.8244 - val_loss: 0.5191 - val_accuracy: 0.8371\n",
            "Epoch 187/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5397 - accuracy: 0.8244 - val_loss: 0.5469 - val_accuracy: 0.8276\n",
            "Epoch 188/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.5477 - accuracy: 0.8219 - val_loss: 0.5017 - val_accuracy: 0.8401\n",
            "Epoch 189/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5385 - accuracy: 0.8238 - val_loss: 0.4974 - val_accuracy: 0.8417\n",
            "Epoch 190/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5397 - accuracy: 0.8231 - val_loss: 0.5089 - val_accuracy: 0.8359\n",
            "Epoch 191/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5366 - accuracy: 0.8256 - val_loss: 0.5257 - val_accuracy: 0.8343\n",
            "Epoch 192/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5308 - accuracy: 0.8251 - val_loss: 0.4986 - val_accuracy: 0.8367\n",
            "Epoch 193/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5309 - accuracy: 0.8259 - val_loss: 0.5216 - val_accuracy: 0.8353\n",
            "Epoch 194/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5295 - accuracy: 0.8272 - val_loss: 0.5332 - val_accuracy: 0.8304\n",
            "Epoch 195/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5347 - accuracy: 0.8263 - val_loss: 0.5319 - val_accuracy: 0.8296\n",
            "Epoch 196/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.5244 - accuracy: 0.8274 - val_loss: 0.5129 - val_accuracy: 0.8371\n",
            "Epoch 197/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5303 - accuracy: 0.8248 - val_loss: 0.5018 - val_accuracy: 0.8416\n",
            "Epoch 198/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5260 - accuracy: 0.8288 - val_loss: 0.4975 - val_accuracy: 0.8419\n",
            "Epoch 199/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5253 - accuracy: 0.8285 - val_loss: 0.5104 - val_accuracy: 0.8367\n",
            "Epoch 200/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5281 - accuracy: 0.8291 - val_loss: 0.5277 - val_accuracy: 0.8327\n",
            "Epoch 201/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5278 - accuracy: 0.8285 - val_loss: 0.5111 - val_accuracy: 0.8395\n",
            "Epoch 202/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5228 - accuracy: 0.8313 - val_loss: 0.5003 - val_accuracy: 0.8416\n",
            "Epoch 203/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5262 - accuracy: 0.8276 - val_loss: 0.5178 - val_accuracy: 0.8375\n",
            "Epoch 204/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5167 - accuracy: 0.8304 - val_loss: 0.4955 - val_accuracy: 0.8421\n",
            "Epoch 205/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5171 - accuracy: 0.8317 - val_loss: 0.4959 - val_accuracy: 0.8433\n",
            "Epoch 206/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5162 - accuracy: 0.8318 - val_loss: 0.4874 - val_accuracy: 0.8465\n",
            "Epoch 207/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5191 - accuracy: 0.8305 - val_loss: 0.4951 - val_accuracy: 0.8435\n",
            "Epoch 208/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5163 - accuracy: 0.8307 - val_loss: 0.5055 - val_accuracy: 0.8405\n",
            "Epoch 209/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5171 - accuracy: 0.8303 - val_loss: 0.4740 - val_accuracy: 0.8495\n",
            "Epoch 210/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5150 - accuracy: 0.8326 - val_loss: 0.4810 - val_accuracy: 0.8471\n",
            "Epoch 211/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5124 - accuracy: 0.8328 - val_loss: 0.5071 - val_accuracy: 0.8419\n",
            "Epoch 212/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.5115 - accuracy: 0.8339 - val_loss: 0.5123 - val_accuracy: 0.8405\n",
            "Epoch 213/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5136 - accuracy: 0.8322 - val_loss: 0.4901 - val_accuracy: 0.8453\n",
            "Epoch 214/250\n",
            "703/703 [==============================] - 24s 33ms/step - loss: 0.5087 - accuracy: 0.8331 - val_loss: 0.4749 - val_accuracy: 0.8512\n",
            "Epoch 215/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5094 - accuracy: 0.8331 - val_loss: 0.5300 - val_accuracy: 0.8355\n",
            "Epoch 216/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.5075 - accuracy: 0.8327 - val_loss: 0.4972 - val_accuracy: 0.8425\n",
            "Epoch 217/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5034 - accuracy: 0.8345 - val_loss: 0.4979 - val_accuracy: 0.8433\n",
            "Epoch 218/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5066 - accuracy: 0.8337 - val_loss: 0.5047 - val_accuracy: 0.8417\n",
            "Epoch 219/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4995 - accuracy: 0.8380 - val_loss: 0.4933 - val_accuracy: 0.8424\n",
            "Epoch 220/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.5014 - accuracy: 0.8367 - val_loss: 0.4669 - val_accuracy: 0.8521\n",
            "Epoch 221/250\n",
            "703/703 [==============================] - 26s 37ms/step - loss: 0.5037 - accuracy: 0.8365 - val_loss: 0.5109 - val_accuracy: 0.8421\n",
            "Epoch 222/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4960 - accuracy: 0.8369 - val_loss: 0.4908 - val_accuracy: 0.8459\n",
            "Epoch 223/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.5015 - accuracy: 0.8363 - val_loss: 0.5239 - val_accuracy: 0.8375\n",
            "Epoch 224/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4957 - accuracy: 0.8380 - val_loss: 0.4975 - val_accuracy: 0.8448\n",
            "Epoch 225/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4992 - accuracy: 0.8368 - val_loss: 0.4796 - val_accuracy: 0.8493\n",
            "Epoch 226/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4971 - accuracy: 0.8378 - val_loss: 0.5472 - val_accuracy: 0.8292\n",
            "Epoch 227/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.4975 - accuracy: 0.8380 - val_loss: 0.4648 - val_accuracy: 0.8528\n",
            "Epoch 228/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4942 - accuracy: 0.8392 - val_loss: 0.4725 - val_accuracy: 0.8507\n",
            "Epoch 229/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4921 - accuracy: 0.8375 - val_loss: 0.5014 - val_accuracy: 0.8444\n",
            "Epoch 230/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4867 - accuracy: 0.8406 - val_loss: 0.5005 - val_accuracy: 0.8437\n",
            "Epoch 231/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4904 - accuracy: 0.8412 - val_loss: 0.5137 - val_accuracy: 0.8401\n",
            "Epoch 232/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4917 - accuracy: 0.8385 - val_loss: 0.4731 - val_accuracy: 0.8513\n",
            "Epoch 233/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4887 - accuracy: 0.8402 - val_loss: 0.4885 - val_accuracy: 0.8459\n",
            "Epoch 234/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4911 - accuracy: 0.8402 - val_loss: 0.4803 - val_accuracy: 0.8497\n",
            "Epoch 235/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4947 - accuracy: 0.8393 - val_loss: 0.4684 - val_accuracy: 0.8536\n",
            "Epoch 236/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4818 - accuracy: 0.8418 - val_loss: 0.4692 - val_accuracy: 0.8536\n",
            "Epoch 237/250\n",
            "703/703 [==============================] - 23s 33ms/step - loss: 0.4821 - accuracy: 0.8426 - val_loss: 0.4932 - val_accuracy: 0.8460\n",
            "Epoch 238/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4861 - accuracy: 0.8408 - val_loss: 0.4935 - val_accuracy: 0.8460\n",
            "Epoch 239/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4857 - accuracy: 0.8408 - val_loss: 0.4929 - val_accuracy: 0.8432\n",
            "Epoch 240/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4843 - accuracy: 0.8421 - val_loss: 0.5290 - val_accuracy: 0.8369\n",
            "Epoch 241/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4832 - accuracy: 0.8441 - val_loss: 0.5025 - val_accuracy: 0.8437\n",
            "Epoch 242/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4904 - accuracy: 0.8382 - val_loss: 0.4738 - val_accuracy: 0.8528\n",
            "Epoch 243/250\n",
            "703/703 [==============================] - 24s 34ms/step - loss: 0.4900 - accuracy: 0.8403 - val_loss: 0.4839 - val_accuracy: 0.8483\n",
            "Epoch 244/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4840 - accuracy: 0.8393 - val_loss: 0.4709 - val_accuracy: 0.8516\n",
            "Epoch 245/250\n",
            "703/703 [==============================] - 28s 40ms/step - loss: 0.4786 - accuracy: 0.8428 - val_loss: 0.4782 - val_accuracy: 0.8533\n",
            "Epoch 246/250\n",
            "703/703 [==============================] - 24s 35ms/step - loss: 0.4814 - accuracy: 0.8429 - val_loss: 0.5354 - val_accuracy: 0.8373\n",
            "Epoch 247/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4784 - accuracy: 0.8424 - val_loss: 0.4925 - val_accuracy: 0.8487\n",
            "Epoch 248/250\n",
            "703/703 [==============================] - 25s 35ms/step - loss: 0.4824 - accuracy: 0.8413 - val_loss: 0.4783 - val_accuracy: 0.8535\n",
            "Epoch 249/250\n",
            "703/703 [==============================] - 25s 36ms/step - loss: 0.4755 - accuracy: 0.8450 - val_loss: 0.4522 - val_accuracy: 0.8565\n",
            "Epoch 250/250\n",
            "703/703 [==============================] - 27s 38ms/step - loss: 0.4777 - accuracy: 0.8448 - val_loss: 0.4786 - val_accuracy: 0.8497\n",
            "704/704 [==============================] - 3s 4ms/step - loss: 226.3452 - accuracy: 0.1359\n",
            "> 13.587\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_pixels2(testX):\n",
        "  # convert from integers to floats\n",
        "  testX_norm = testX.astype('float32')\n",
        "  # normalize to range 0-1\n",
        "  testX_norm = testX_norm / 255.0\n",
        "  \n",
        "  # return normalized images\n",
        "  return testX_norm"
      ],
      "metadata": {
        "id": "OwcjwLRtiYKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc = model.evaluate(prep_pixels2(e),f, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MO6nMI7hjFV",
        "outputId": "74d8b1e3-b6df-47de-f80e-9e8b9582eada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "704/704 [==============================] - 3s 4ms/step - loss: 0.4755 - accuracy: 0.8492\n"
          ]
        }
      ]
    }
  ]
}